{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea2ea3b1",
   "metadata": {},
   "source": [
    "# TinyML Autoencoder for ToyADMOS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28325ec",
   "metadata": {},
   "source": [
    "Start by getting the data into the notebook. We will focus on the first case of toy car example, and use a subset of that dataset to train an autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bd248d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9caf2b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_path = \"/Users/emjn/Documents/DTU/Datasets/ToyConveyor/case1/NormalSound_IND/\"\n",
    "test_path = \"/Users/emjn/Documents/DTU/Datasets/ToyConveyor/case1/AnomalousSound_IND/\"\n",
    "\n",
    "# Get the file paths for the sound files in the training and test path\n",
    "training_files_path = tf.io.gfile.glob(training_path + \"*ch1*.wav\")\n",
    "test_files_path = tf.io.gfile.glob(test_path + \"*ch1*.wav\")\n",
    "\n",
    "# Let us reduce the amount of training and test samples for the moment\n",
    "training_files_path = training_files_path[:1000]\n",
    "test_files_path = test_files_path[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97e2e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is very slow, maybe it could be optimized\n",
    "training_audio = []\n",
    "test_audio = []\n",
    "\n",
    "_, sr = librosa.load(training_files_path[0])\n",
    "\n",
    "for audio_file in training_files_path:\n",
    "    sample, _ = librosa.load(audio_file)\n",
    "    training_audio.append(sample)\n",
    "    \n",
    "for audio_file in test_files_path:\n",
    "    sample, _ = librosa.load(audio_file)\n",
    "    test_audio.append(sample)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4178b74c",
   "metadata": {},
   "source": [
    "Now that we have the audio loaded into memory, we create spectrograms based on the audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b816f860",
   "metadata": {},
   "outputs": [],
   "source": [
    "FRAME_SIZE = 2048\n",
    "HOP_SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d812cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_stft(audio_sample):\n",
    "    stft = librosa.stft(audio_sample, n_fft=FRAME_SIZE, hop_length=HOP_SIZE)\n",
    "    magnitude = np.abs(stft) ** 2\n",
    "    return librosa.power_to_db(magnitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3239c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_magnitudes = []\n",
    "test_magnitudes = []\n",
    "\n",
    "for audio in training_audio:\n",
    "    training_magnitudes.append(apply_stft(audio))\n",
    "for audio in test_audio:\n",
    "    test_magnitudes.append(apply_stft(audio))\n",
    "    \n",
    "training_magnitudes = np.array(training_magnitudes)\n",
    "test_magnitudes = np.array(test_magnitudes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba6291c",
   "metadata": {},
   "source": [
    "Let us visualize a spectrogram of one of the sound files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b5d150",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25,10))\n",
    "librosa.display.specshow(training_magnitudes[2], sr=sr, hop_length=HOP_SIZE, x_axis=\"time\", y_axis=\"log\")\n",
    "plt.colorbar(format=\"%+2.f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3dded7",
   "metadata": {},
   "source": [
    "## Defining the model\n",
    "Having generated the spectrograms we now need to preprocess the them to be suitable for the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5371fd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_magnitudes.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ed91da",
   "metadata": {},
   "source": [
    "We need to add a new dimension to the datasets to be able to work with tensorflows convolutions that expect dimensions that would normally be present in an image (dataset_dimension, x, y, channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e13621",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_magnitudes_4D = training_magnitudes[..., tf.newaxis]\n",
    "test_magnitudes_4D = test_magnitudes[..., tf.newaxis]\n",
    "\n",
    "# We pad the spectrograms to multiples of 4 to make max pooling and upsampling result in the same shape\n",
    "x_train = tf.keras.layers.ZeroPadding2D(padding=((3,0),(1,0)))(training_magnitudes_4D)\n",
    "x_test = tf.keras.layers.ZeroPadding2D(padding=((3,0),(1,0)))(test_magnitudes_4D)\n",
    "\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2632d1d1",
   "metadata": {},
   "source": [
    "As it can be seen on the spectrogram plotted above, the current magnitudes range from about +35 to -45. It is problematic that the values include negative numbers as it will not be possible to reconstruct these using the ReLU activation function. Furthermore neural networks work the best in values ranging from 0-1.\n",
    "\n",
    "Therefore we apply a shift and a scale to keep the magnitudes of the spectrograms inside this range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce711245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the min and max value to compute the shift and scale\n",
    "min_value = tf.reduce_min(x_train)\n",
    "max_value = tf.reduce_max(x_train)\n",
    "\n",
    "# Apply shift\n",
    "x_train = tf.math.subtract(x_train, min_value)\n",
    "x_test = tf.math.subtract(x_test, min_value)\n",
    "\n",
    "# Apply scale\n",
    "\n",
    "x_train = x_train / max_value\n",
    "x_test = x_test / max_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa2b847",
   "metadata": {},
   "source": [
    "## Defining the model\n",
    "We now define an autoencoder to take the preprocessed spectrograms as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f7de12",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = tf.keras.models.Sequential()\n",
    "\n",
    "encoder.add(tf.keras.layers.Conv2D(32, 3, padding=\"same\",\n",
    "                                   activation=\"relu\", input_shape=x_train.shape[1:]))\n",
    "encoder.add(tf.keras.layers.MaxPooling2D())\n",
    "\n",
    "encoder.add(tf.keras.layers.Conv2D(64, 3, padding=\"same\", activation=\"relu\"))\n",
    "encoder.add(tf.keras.layers.MaxPooling2D())\n",
    "\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4674781",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = tf.keras.models.Sequential()\n",
    "\n",
    "decoder.add(tf.keras.layers.Conv2D(32, 3, padding=\"same\", activation=\"relu\", input_shape=encoder.output.shape[1:]))\n",
    "decoder.add(tf.keras.layers.UpSampling2D())\n",
    "\n",
    "decoder.add(tf.keras.layers.Conv2D(16, 3, padding=\"same\", activation=\"relu\", input_shape=encoder.output.shape[1:]))\n",
    "decoder.add(tf.keras.layers.UpSampling2D())\n",
    "\n",
    "decoder.add(tf.keras.layers.Conv2D(1, 3, padding=\"same\", activation=\"relu\", input_shape=encoder.output.shape[1:]))\n",
    "\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dabeb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_autoencoder = tf.keras.Model(inputs=encoder.input, outputs=decoder(encoder.outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110ddf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_autoencoder.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.MeanSquaredError())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6381d7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = conv_autoencoder.fit(x_train, x_train, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10a2985",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"loss\"], label=\"Training Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002f6416",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructions = conv_autoencoder.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1944e45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "squeezed_reconstructions = tf.squeeze(reconstructions)\n",
    "squeezed_reconstructions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c296fe",
   "metadata": {},
   "source": [
    "To get back to the original spectrogram scale we shift and scale the reconstructed spectrograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f69be48",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_reconstructions = squeezed_reconstructions * max_value\n",
    "\n",
    "shifted_reconstructions = tf.math.add(scaled_reconstructions, min_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d68679",
   "metadata": {},
   "source": [
    "### An original spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fca1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which spectrogram to visualize\n",
    "spec_number=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a292c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25,10))\n",
    "librosa.display.specshow(test_magnitudes[spec_number], sr=sr, hop_length=HOP_SIZE, x_axis=\"time\", y_axis=\"log\")\n",
    "plt.colorbar(format=\"%+2.f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df700886",
   "metadata": {},
   "source": [
    "### The reconstructed image\n",
    "There seems to be an issue with the autoencoder not generating negative numbers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cb7bc9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25,10))\n",
    "librosa.display.specshow(shifted_reconstructions[spec_number].numpy(), sr=sr, hop_length=HOP_SIZE, x_axis=\"time\", y_axis=\"log\")\n",
    "plt.colorbar(format=\"%+2.f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcf54f7",
   "metadata": {},
   "source": [
    "## We now have to define a treshold for when to reject a sample as normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23e5183",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_reconstructions = reconstructions\n",
    "\n",
    "# I implement a for loop to get the mean squared error for every prediction.\n",
    "# This should also be possible to do with reduction types over a dataset.\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/losses/MeanSquaredError\n",
    "mse = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.AUTO)\n",
    "\n",
    "normal_losses = []\n",
    "for i in range(len(x_train)):\n",
    "    normal_losses.append(mse(normal_reconstructions[i], x_train[i]))\n",
    "\n",
    "plt.hist(tf.convert_to_tensor(normal_losses)[None,:], bins=10)\n",
    "plt.xlabel(\"Train loss\")\n",
    "plt.ylabel(\"No of examples\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b113693f",
   "metadata": {},
   "source": [
    "It seems that the loss of normal samples are around 0.028. Lets try to use the test set to see if we see higher reconstruction errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e51eee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "abnormal_reconstructions = conv_autoencoder.predict(x_test)\n",
    "\n",
    "abnormal_losses = []\n",
    "for i in range(len(x_test)):\n",
    "    abnormal_losses.append(mse(abnormal_reconstructions[i], x_test[i]))\n",
    "\n",
    "plt.hist(tf.convert_to_tensor(abnormal_losses)[None,:], bins=10)\n",
    "plt.xlabel(\"Train loss\")\n",
    "plt.ylabel(\"No of examples\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f1afce",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(abnormal_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea04b202",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(normal_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e8273e",
   "metadata": {},
   "source": [
    "Currently the model does not seem very useful as the mean loss of the abnormal sounds are less than that of the normal sounds. We still try to define a treshhold that could be used in a better model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907f470a",
   "metadata": {},
   "outputs": [],
   "source": [
    "treshold = np.mean(normal_losses) + np.std(normal_losses)\n",
    "print(\"Treshold:\", treshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "635fae01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data, treshold):\n",
    "    reconstructions = model(data)\n",
    "    loss = tf.keras.losses.mse(reconstructions, data)\n",
    "    return tf.math.less(loss, treshold)\n",
    "\n",
    "def print_stats(predictions, labels):\n",
    "    print(\"Accuracy = {}\".format(accuracy_score(labels, predictions)))\n",
    "    print(\"Precision = {}\".format(precision_score(labels, predictions)))\n",
    "    print(\"Recall = {}\".format(recall_score(labels, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7121d8cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Exception encountered when calling layer \"conv2d\" (type Conv2D).\n\nOOM when allocating tensor with shape[400,1028,432,32] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator Simple allocator [Op:Conv2D]\n\nCall arguments received:\n  • inputs=tf.Tensor(shape=(400, 1028, 432, 1), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/zl/69pfzz754394ftnkk3zd04t40000gn/T/ipykernel_1116/423252051.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv_autoencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtreshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/zl/69pfzz754394ftnkk3zd04t40000gn/T/ipykernel_1116/1728103424.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(model, data, treshold)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtreshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mreconstructions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreconstructions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mless\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtreshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tensorflowM1/lib/python3.9/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tensorflowM1/lib/python3.9/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7105\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7106\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7107\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: Exception encountered when calling layer \"conv2d\" (type Conv2D).\n\nOOM when allocating tensor with shape[400,1028,432,32] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator Simple allocator [Op:Conv2D]\n\nCall arguments received:\n  • inputs=tf.Tensor(shape=(400, 1028, 432, 1), dtype=float32)"
     ]
    }
   ],
   "source": [
    "predictions = predict(conv_autoencoder, x_test, treshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bc8a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_stats(predictions, tf.zeroes_like(x_test).astype(bool))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit ('tensorflowM1': conda)",
   "language": "python",
   "name": "python3910jvsc74a57bd033c8af7123734faadca281f791e7fcb11f354e7831eca1cdbd3d17d545a59677"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
