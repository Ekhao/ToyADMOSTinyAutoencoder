{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f250d57-83cb-4a75-aad3-6f426308a571",
   "metadata": {},
   "source": [
    "# Load a saved tensorflow model and run inference. (Autoencoder on ToyADMOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "753903a1-1d80-4285-9071-7bed6d5faa8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRESHOLD_FOR_MODEL = 0.017349888\n",
    "MAX_VALUE_FOR_MODEL = 21.5335\n",
    "MIN_VALUE_FOR_MODEL = -62.62027"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d581fe99-e09d-4c6f-9866-8ee2df32005e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6574e2a3-65bf-4351-8153-809f1a7f8880",
   "metadata": {},
   "source": [
    "We first load the model from the saved_tf_models directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90f6e53c-58a5-410a-a24e-985f3b353beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"./saved_tf_models/test/\"\n",
    "conv_autoencoder = tf.keras.models.load_model(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd18684-96f0-4d7f-96f0-bbe91189c9f0",
   "metadata": {},
   "source": [
    "## We now have to prepare the ToyADMOS audio to input to the saved model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29ae675-f087-42eb-a3b6-f21b2705a4b0",
   "metadata": {},
   "source": [
    "This is the same preprocessing done in the original notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0be38fb5-6c09-4ee4-944b-4970d3802092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is 1800 normal sound files and 400 anomalous sound files \n",
    "NUMBER_OF_NORMAL_FILES = 40\n",
    "NUMBER_OF_ANOMALOUS_FILES = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc036694-70aa-462b-b26d-7f7fc9b66445",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_path = \"/Users/emjn/Documents/DTU/Datasets/ToyConveyor/case1/NormalSound_IND/\"\n",
    "anomalous_path = \"/Users/emjn/Documents/DTU/Datasets/ToyConveyor/case1/AnomalousSound_IND/\"\n",
    "\n",
    "normal_files_path = tf.io.gfile.glob(normal_path + \"*ch1*.wav\")\n",
    "anomalous_files_path = tf.io.gfile.glob(anomalous_path + \"*ch1*.wav\")\n",
    "\n",
    "normal_files_path = tf.convert_to_tensor(normal_files_path[:NUMBER_OF_NORMAL_FILES])\n",
    "anomalous_files_path = tf.convert_to_tensor(anomalous_files_path[:NUMBER_OF_ANOMALOUS_FILES])\n",
    "\n",
    "audio_file = normal_files_path[0].numpy()\n",
    "_, sr = librosa.load(audio_file)\n",
    "\n",
    "def custom_librosa_load(audio_file):\n",
    "    audio, _ = librosa.load(audio_file.numpy())\n",
    "    return audio\n",
    "\n",
    "normal_audio = tf.map_fn(fn=custom_librosa_load, elems=normal_files_path, fn_output_signature=tf.float32)\n",
    "anomalous_audio = tf.map_fn(fn=custom_librosa_load, elems=anomalous_files_path, fn_output_signature=tf.float32)\n",
    "\n",
    "FRAME_SIZE = 2048\n",
    "HOP_SIZE = 512\n",
    "\n",
    "def apply_stft(audio_sample):\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(audio_sample.numpy(), sr=sr, n_fft=2048, hop_length=512, n_mels=256)\n",
    "    return librosa.power_to_db(mel_spectrogram)\n",
    "\n",
    "normal_magnitudes = tf.map_fn(fn=apply_stft, elems=normal_audio)\n",
    "anomalous_magnitudes = tf.map_fn(fn=apply_stft, elems=anomalous_audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f1f909-fcfa-4aac-ab65-e196ba806da7",
   "metadata": {},
   "source": [
    "We now have to create a dataset to run inference on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aaab26d4-b5a9-4707-a22b-23285b546cd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "magnitudes = tf.concat([normal_magnitudes, anomalous_magnitudes], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cf0028e1-7b60-4a64-90d1-74c00be6e11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "magnitudes_4D = test_magnitudes[..., tf.newaxis]\n",
    "\n",
    "x = tf.keras.layers.ZeroPadding2D(padding=((0,0),(1,0)))(magnitudes_4D)\n",
    "\n",
    "# Apply shift\n",
    "x = tf.math.subtract(x, MIN_VALUE_FOR_MODEL)\n",
    "\n",
    "# Apply scale\n",
    "x = x / MAX_VALUE_FOR_MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c164e0-8dcd-4bd5-b9df-6ae3aca7b703",
   "metadata": {},
   "source": [
    "## Running inference using the loaded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a93c22e6-cab0-4935-98bf-0aa1a64336a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = conv_autoencoder.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8f763ff8-7d3a-44c4-89b4-73df82abe624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 256, 432, 1)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3eb153bf-105e-443a-b513-4ab31a681de0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([80, 256, 432, 1])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2e0d859c-20e0-4ff4-853c-3c4e7e126371",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped_x = tf.reshape(x, [x.shape[0], tf.math.reduce_prod(x.shape[1:])])\n",
    "reshaped_predictions = tf.reshape(predictions, [predictions.shape[0], tf.math.reduce_prod(predictions.shape[1:])])\n",
    "loss = tf.keras.losses.mse(reshaped_x, reshaped_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2db3e792-7e74-4193-8c50-0fdbf021aece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(80,), dtype=bool, numpy=\n",
       "array([ True,  True, False,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True, False,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False])>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.math.less(loss, TRESHOLD_FOR_MODEL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit ('tensorflowM1': conda)",
   "language": "python",
   "name": "python3910jvsc74a57bd033c8af7123734faadca281f791e7fcb11f354e7831eca1cdbd3d17d545a59677"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
